{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Profundo ou Deep Learning \n",
    "\n",
    "## O que é:\n",
    "Deep learning é uma subárea da inteligência artificial (IA) e do machine learning (aprendizado de máquina) que se concentra no uso de redes neurais artificiais profundas para resolver problemas complexos.\n",
    "\n",
    "## Como funciona:\n",
    "O deep learning utiliza redes neurais profundas para processar os dados. Essas redes consistem em neurônios artificiais organizados em camadas interconectadas.\n",
    "\n",
    "Cada camada recebe informações da anterior, transforma-as de acordo com parâmetros aprendidos durante o treinamento e passa os resultados adiante.\n",
    "\n",
    "O treinamento de uma rede neural é feito usando algoritmos de aprendizado, como o backpropagation (retropropagação), que ajusta os pesos das conexões entre os neurônios para minimizar os erros entre as previsões do modelo e os resultados reais. \n",
    "\n",
    "## Diferença entre machine learning e deep learning:\n",
    "→ Machine learning convencional geralmente depende da extração manual de recursos (informações sobre o problema). \n",
    "\n",
    "→ Deep learning elimina a necessidade de extrair manualmente características dos dados.\n",
    "\n",
    "## Diferença entre deep learning e redes neurais:\n",
    "→ Redes neurais são o conceito fundamental, representando um modelo matemático inspirado no \n",
    "funcionamento do cérebro humano.\n",
    "\n",
    "→ Deep learning é uma aplicação específica das redes neurais, que utiliza muitas camadas para realizar aprendizado profundo.\n",
    "\n",
    "⚠️ toda técnica de deep learning utiliza redes neurais, mas nem toda rede neural é usada em deep learning.\n",
    "\n",
    "## Tipos de algoritmos de deep learning:\n",
    "* Redes neurais convolucionais (CNN) → p/ imagens (reconhecimento facial, detecção de objetos e análise de imagens médicas)\n",
    "* Redes neurais recorrentes (RNN) → p/ palavras (séries temporais, texto ou áudio)\n",
    "* Redes adversárias generativas (GAN) → p/ deepfakes (imagens, vídeos, músicas ou textos)\n",
    "* Modelos de difusão → p/ geração (imagens, áudio e até design molecular)\n",
    "* Modelos transformadores → p/ ChatGPT e Google Gemini (sequências de dados, como texto, áudio e séries temporais)\n",
    "\n",
    "## Por que usar o deep learning?\n",
    "→ Para analisar dados não estruturados com eficiência: \n",
    "* Machine learning é comum trabalharmos com dados estruturados, tabelas com informações organizadas por colunas.\n",
    "* Deep learning se destaca por sua habilidade de lidar com dados não estruturados, como imagens, áudios, textos e vídeos, que representam a maior parte das informações disponíveis no mundo real.\n",
    "\n",
    "## Referências:\n",
    "* https://www.alura.com.br/artigos/deep-learning?srsltid=AfmBOooSLdvjpGMPscvEjYUV2DSGTaYsRP7lAXVjKxTWwcF0tL8dnmP1\n",
    "* https://didatica.tech/introducao-a-redes-neurais-e-deep-learning/#:~:text=Deep%20Learning%20x%20Redes%20Neurais&text=Conforme%20j%C3%A1%20mencionamos%2C%20o%20deep,n%C3%A3o%20estamos%20utilizando%20deep%20learning.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter os dados e já dividi-los em treinamento e teste automaticamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_treinamento, y_treinamento), (X_teste, y_teste) = mnist.load_data() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibir um item da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_treinamento[21], cmap='gray')\n",
    "plt.title(y_treinamento[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudar dimensão para 784 (original está em 28x28)\n",
    "Seguir as etapas de transformação de ↑→ para →, que são comuns no pré-processamento de dados para tarefas de aprendizado profundo, especialmente quando se trabalha com imagens. \n",
    "\n",
    "\n",
    "Na base de dados que está armazenada na variável `X_treinamento`\n",
    "\n",
    "Passo 1: Entender a distribuição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (60000, 28, 28)\n",
      "Qtd linhas:  60000\n",
      "Qtd colunas:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma: \", X_treinamento.shape)\n",
    "print(\"Qtd linhas: \", len(X_treinamento))\n",
    "print(\"Qtd colunas: \", X_treinamento.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Redimensionamento dos dados (reshape):\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "Em muitos modelos de aprendizado profundo, especialmente redes neurais densas (totalmente conectadas), a entrada precisa ser um vetor unidimensional. Imagens, por outro lado, são dados multidimensionais (largura, altura e canais de cor).\n",
    "\n",
    "O reshape transforma cada imagem em um longo vetor, \"achatando\" a matriz multidimensional em um vetor unidimensional. Isso torna os dados compatíveis com a camada de entrada da rede neural.\n",
    "\n",
    "np.prod(X_treinamento.shape[1:]) calcula o número total de elementos em todas as dimensões, exceto a primeira (que geralmente representa o número de amostras). \n",
    "\n",
    "Isso garante que o vetor resultante tenha o tamanho correto.Transformar de uma imagem quadrada ↑→ para uma imagem linha →"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento = X_treinamento.reshape((len(X_treinamento), np.prod(X_treinamento.shape[1:])))\n",
    "#X_treinamento.shape[1:] → Pega todas as dimensões depois da primeira. No caso (60000, 28, 28), isso retorna (28, 28).\n",
    "#np.prod((28, 28)) → Multiplica 28 × 28 = 784. Isso significa que cada imagem de 28×28 será achatada em um vetor de 784 elementos.\n",
    "#reshape → Isso transforma cada imagem 28×28 em um vetor unidimensional de 784 valores (achatando os dados), mantendo o mesmo número de amostras (60000).\n",
    "\n",
    "#Observação: Se estivéssemos lidando com imagens coloridas (RGB 28×28×3), o código adaptaria automaticamente para 28×28×3 = 2352 entradas.\n",
    "\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Conversão para float32 (para possibilitar a normalização [de 0 a 1] dos dados)\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "A normalização [de 0 a 1], que vem na próxima etapa, envolve dividir os valores dos pixels por 255. \n",
    "\n",
    "Essa divisão requer que os dados sejam do tipo float (número de ponto flutuante) para evitar arredondamentos indesejados.\n",
    "\n",
    "float32 é um tipo de dado de ponto flutuante de 32 bits, que oferece precisão suficiente para a maioria das aplicações de aprendizado profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento = X_treinamento.astype('float32')\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 4: Normalizar os dados (255 é o valor máximo de um pixel)\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "A normalização é uma técnica crucial para melhorar o desempenho e a estabilidade do treinamento de redes neurais.\n",
    "\n",
    "Dividir os valores dos pixels por 255 escala os dados para o intervalo de 0 a 1. \n",
    "\n",
    "Isso garante que todos os recursos (pixels) tenham uma escala semelhante, o que facilita o aprendizado da rede.\n",
    "\n",
    "A normalização ajuda a evitar que grandes valores de entrada dominem o processo de treinamento e pode acelerar a convergência do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento /= 255  #Divide por 255 para normalizar os valores entre 0 e 1\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 5: Transformar as 10 classes de saída da variável `y_treinamento` para dummies variables (one-hot encoding):\n",
    "\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "Em tarefas de classificação multiclasse, como a classificação de dígitos (onde você tem 10 classes ), a variável de destino (rótulos) precisa ser representada de forma adequada para a rede neural.\n",
    "\n",
    "to_categorical realiza o one-hot encoding, que transforma cada rótulo em um vetor binário. Por exemplo, o rótulo \"3\" seria transformado em um vetor com 10 elementos, onde o quarto elemento é 1 e os demais são 0.\n",
    "\n",
    "* [0, 1, 2, **3**, 4, 5, 6, 7, 8 e 9]\n",
    "* [0, 0, 0, **1**, 0, 0, 0, 0, 0 e 0]\n",
    "\n",
    "Essa representação permite que a rede neural produza probabilidades para cada classe e facilita o cálculo da função de perda durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_treinamento = to_categorical(y_treinamento, 10)\n",
    "y_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer o mesmo na base de dados que está armazenada na variável `X_teste`\n",
    "\n",
    "Passo 1: Entender a distribuição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (10000, 28, 28)\n",
      "Qtd linhas:  10000\n",
      "Qtd colunas:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma: \", X_teste.shape)\n",
    "print(\"Qtd linhas: \", len(X_teste))\n",
    "print(\"Qtd colunas: \", X_teste.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2:  Redimensionamento dos dados (reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 222, 254, 254, 254,\n",
       "       254, 241, 198, 198, 198, 198, 198, 198, 198, 198, 170,  52,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  67, 114,\n",
       "        72, 114, 163, 227, 254, 225, 254, 254, 254, 250, 229, 254, 254,\n",
       "       140,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  17,  66,  14,  67,  67,  67,  59,  21,\n",
       "       236, 254, 106,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  83, 253, 209,  18,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 133, 254, 187,   5,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   9, 205, 248,  58,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 126, 254, 182,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  75, 251,\n",
       "       240,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "       221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         3, 203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  38, 254, 254,  77,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  31, 224, 254, 115,   1,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 133, 254, 254,  52,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  61, 242, 254, 254,  52,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254, 219,  40,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
       "        18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Faz a mesma coisa com a base acima, porém com a partição de teste\n",
    "X_teste = X_teste.reshape((len(X_teste), np.prod(X_teste.shape[1:])))\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Conversão para float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  84., 185., 159., 151.,  60.,  36.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 222.,\n",
       "       254., 254., 254., 254., 241., 198., 198., 198., 198., 198., 198.,\n",
       "       198., 198., 170.,  52.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  67., 114.,  72., 114., 163., 227.,\n",
       "       254., 225., 254., 254., 254., 250., 229., 254., 254., 140.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  17.,  66.,  14.,  67.,  67.,  67.,\n",
       "        59.,  21., 236., 254., 106.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  83., 253., 209.,  18.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  22., 233., 255.,  83.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 129., 254., 238.,  44.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  59., 249., 254.,  62.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 133., 254., 187.,   5.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   9., 205., 248.,  58.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 126., 254., 182.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  75., 251., 240.,  57.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  19., 221., 254., 166.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         3., 203., 254., 219.,  35.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  38., 254., 254.,  77.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        31., 224., 254., 115.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0., 133., 254., 254.,  52.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        61., 242., 254., 254.,  52.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0., 121., 254., 254., 219.,  40.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., 121., 254., 207.,  18.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste = X_teste.astype('float32')\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 4: Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste /= 255  #Divide por 255 para normalizar os valores entre 0 e 1\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 5: Transformação das classes em dummies variables (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste = to_categorical(y_teste, 10)\n",
    "y_teste[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar a estrutura da rede neural:\n",
    "`Entrada (784)` → `Camada oculta (64)` → `Camada oculta (64)` → `Camada oculta (64)` → `Saída (10)`\n",
    "\n",
    "\n",
    "* 3 camadas ocultas com 64 neurônios cada e ativação ReLU.\n",
    "* Dropout (20%) é utilizado para zerar o um percentual de neurônios após cada camada oculta para evitar overfitting.\n",
    "* Camada de saída com 10 neurônios e ativação Softmax, ideal para classificação multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gui\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Armazena na variável o modelo Sequencial (feedforward), onde as camadas são adicionadas uma após a outra (em sequência)\n",
    "modelo = Sequential() \n",
    "\n",
    "modelo.add(Dense(units = 64, # Adiciona uma camada densa totalmente conectada com 64 neurônios\n",
    "                 activation = 'relu', # ReLU (Rectified Linear Unit 𝑓(𝑥)=max⁡(0,𝑥) melhora o aprendizado e evita saturação como o sigmoid (vanishing gradient)\n",
    "                 input_dim = 784)) # Entrada tem 784 neurônios (útil para imagens 28×28 achatadas)\n",
    "modelo.add(Dropout(0.2)) # Zera aleatoriamente 20% dos neurônios durante o treinamento evitando overfitting (forçando a rede a não depender excessivamente de neurônios específicos)\n",
    "\n",
    "modelo.add(Dense(units = 64, activation = 'relu')) # Outra camada densa com 64 neurônios e ativação ReLU\n",
    "modelo.add(Dropout(0.2)) # Dropout aplicado novamente para regularizar a rede\n",
    "\n",
    "modelo.add(Dense(units = 64, activation = 'relu')) # Igual\n",
    "modelo.add(Dropout(0.2)) # Igual\n",
    "\n",
    "modelo.add(Dense(units = 10, activation = 'softmax')) # Camada com 10 neurônios, pois temos 10 classes (dígitos de 0 a 9)\n",
    "# A função softmax converte os valores em probabilidades, garantindo que a soma seja 1, permitindo que o modelo escolha a classe (de 0 a 9) com maior probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo da arquitetura da rede neural gerado pelo comando modelo.summary():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "1. **Modelo:** sequential_1\n",
    "   * Modelos sequenciais são uma pilha linear de camadas, onde cada camada tem exatamente um tensor de **entrada** e um tensor de **saída**:  \n",
    "     * `Entrada (784)`** → `Densa (64)` → `Dropout (0.2)` → `Densa (64)` → `Dropout (0.2)` → `Densa (64)` → `Dropout (0.2)` → **`Saída (10)`**\n",
    "\n",
    "2. **Layer (type)** - Camadas: \n",
    "* **Dense:** `dense_4`, `dense_5`, `dense_6`, `dense_7`\n",
    "  * Essas são camadas densas (ou totalmente conectadas). Em uma camada densa, cada neurônio está conectado a todos os neurônios da camada anterior.\n",
    "  * Cada camada densa realiza uma transformação linear nos dados de entrada, seguida por uma função de ativação (que não é mostrada aqui, mas é crucial).\n",
    "  * As camadas densas são responsáveis por aprender padrões complexos nos dados.\n",
    "  * Como os parâmetros são calculados: Cada camada Densa (Dense) tem dois tipos de parâmetros treináveis\n",
    "    * Pesos (weights): Cada conexão entre os neurônios.\n",
    "    * Vieses (biases): Um valor adicional para cada neurônio.\n",
    "  * A fórmula para calcular o número total de parâmetros em uma camada densa é: \n",
    "    * $Parâmetros=(neurônios da camada anterior * neurônios da camada atual) + neurônios da camada atual$\n",
    "* **Dropout:** `dropout_3`, `dropout_4`, `dropout_5`\n",
    "  * O dropout é uma técnica de regularização que desativa aleatoriamente uma fração dos neurônios durante o treinamento.\n",
    "  * Isso ajuda a prevenir o overfitting (quando o modelo se ajusta demais aos dados de treinamento e não generaliza bem para novos dados).\n",
    "\n",
    "3. **Output Shape** - Formato de Saída\n",
    "  * `(None, 64)` significa que a saída de cada uma das primeiras camadas densas e dropout tem esse formato \n",
    "    * `None` representa o tamanho do lote (batch size), que pode variar.\n",
    "    * `64` indica que cada camada densa tem 64 neurônios.\n",
    "   * `(None, 10)` na camada de saída (`dense_7`) tem 10 neurônios, o que sugere que este modelo está sendo usado para uma tarefa de classificação com 10 classes.\n",
    "\n",
    "4. Param # - Parâmetros\n",
    "  * **Cálculo dos valores exibidos:** esses números representam a quantidade de parâmetros (pesos e bias) em cada camada densa\n",
    "    *  1️⃣ Primeira camada densa (`dense_4`): $(784×64)+64=50.240$\n",
    "    *  2️⃣ Segunda camada densa (`dense_5`): $(64×64)+64=4.160$\n",
    "    *  3️⃣ Terceira camada densa (`dense_6`): $(64×64)+64=4.160$\n",
    "    *  4️⃣ Camada de saída (`dense_7`): $(64×10)+10=650$\n",
    "    *  ✅ Total de parâmetros treináveis = $59.210$\n",
    "   * As camadas de dropout não possuem parametros treinaveis, dessa forma o parametro é $0$.\n",
    "   * Quanto mais parâmetros, mais complexo é o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar os parâmetros da rede neural e treiná-la utilizando a base de dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', # Otimizador Adam é uma variação do gradiente descendente que se adapta à taxa de aprendizado\n",
    "               loss='categorical_crossentropy', # Função de perda para classificação multiclasse\n",
    "               metrics=['accuracy']) # Métrica para avaliar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenar o histórico das execuções (erro e accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7501 - loss: 0.7668 - val_accuracy: 0.9466 - val_loss: 0.1765\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9287 - loss: 0.2438 - val_accuracy: 0.9580 - val_loss: 0.1322\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9432 - loss: 0.2000 - val_accuracy: 0.9659 - val_loss: 0.1122\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9498 - loss: 0.1715 - val_accuracy: 0.9651 - val_loss: 0.1111\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9534 - loss: 0.1577 - val_accuracy: 0.9688 - val_loss: 0.1087\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9581 - loss: 0.1441 - val_accuracy: 0.9680 - val_loss: 0.1020\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1344 - val_accuracy: 0.9690 - val_loss: 0.1030\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9631 - loss: 0.1236 - val_accuracy: 0.9703 - val_loss: 0.0995\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9658 - loss: 0.1137 - val_accuracy: 0.9711 - val_loss: 0.0977\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9648 - loss: 0.1190 - val_accuracy: 0.9730 - val_loss: 0.0898\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.1121 - val_accuracy: 0.9712 - val_loss: 0.0942\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9682 - loss: 0.1080 - val_accuracy: 0.9732 - val_loss: 0.0946\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.1064 - val_accuracy: 0.9710 - val_loss: 0.0969\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9692 - loss: 0.1014 - val_accuracy: 0.9728 - val_loss: 0.0872\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9695 - loss: 0.1021 - val_accuracy: 0.9725 - val_loss: 0.0928\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0977 - val_accuracy: 0.9746 - val_loss: 0.0896\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.0936 - val_accuracy: 0.9726 - val_loss: 0.0940\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9714 - loss: 0.0952 - val_accuracy: 0.9747 - val_loss: 0.0916\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9718 - loss: 0.0926 - val_accuracy: 0.9760 - val_loss: 0.0862\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9733 - loss: 0.0882 - val_accuracy: 0.9750 - val_loss: 0.0920\n"
     ]
    }
   ],
   "source": [
    "historico = modelo.fit(X_treinamento, y_treinamento, epochs = 20, validation_data = (X_teste, y_teste)) # Treina o modelo com 20 épocas e valida com a partição de teste\n",
    "\n",
    "# Esquerda  | Direita "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar Accuracy e Loss em um gráfico\n",
    "\n",
    "Interpretação: o erro (embaixo) foi caindo e a acurácia foi subindo (em cima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25bf0352fd0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "historico.history.keys() # Mostra as métricas disponíveis\n",
    "\n",
    "plt.subplot(1, 2, 1) # ← (linhas, colunas, posição exibida 1)\n",
    "plt.title('Acurácia e Perda de Treinamento') # Título do gráfico\n",
    "plt.plot(historico.history['accuracy']) # Acurácia de treinamento\n",
    "plt.plot(historico.history['loss']) # Perda de treinamento \n",
    "\n",
    "plt.subplot(1, 2, 2) # ← (linhas, colunas, posição exibida 2)\n",
    "plt.title('Acurácia e Perda de Validação') # Título do gráfico\n",
    "plt.plot(historico.history['val_accuracy']) # Acurácia de validação\n",
    "plt.plot(historico.history['val_loss']) # Perda de validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.28778915e-09, 3.62624575e-09, 3.36466910e-05, ...,\n",
       "        9.99950528e-01, 3.66337204e-11, 1.56653387e-05],\n",
       "       [3.18219673e-09, 1.17445691e-03, 9.98679817e-01, ...,\n",
       "        6.97384576e-06, 7.07281913e-08, 3.43307535e-11],\n",
       "       [8.83446927e-11, 9.99982238e-01, 1.80028019e-06, ...,\n",
       "        3.37255869e-06, 1.10600195e-05, 1.06281979e-08],\n",
       "       ...,\n",
       "       [1.12036137e-10, 3.22087287e-11, 2.03943662e-09, ...,\n",
       "        2.58493628e-07, 1.80525091e-11, 2.40384725e-06],\n",
       "       [5.22574760e-17, 4.36005509e-15, 2.22647354e-15, ...,\n",
       "        2.75790984e-15, 3.15034998e-09, 7.19939343e-12],\n",
       "       [1.27309727e-08, 1.85595718e-15, 2.09742779e-10, ...,\n",
       "        1.77294065e-11, 4.38389602e-11, 7.24694718e-13]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = modelo.predict(X_teste) # Faz previsões com a partição de teste para avaliar o modelo \n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ...,  True, False, False],\n",
       "       [False, False,  True, ..., False, False, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = (previsoes > 0.5) # Converte as probabilidades em True (1) ou False (0)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegar o número de maior valor e gerar a Matriz de Confusão\n",
    "\n",
    "O código tem como objetivo avaliar o desempenho de um modelo de classificação que prevê dígitos de 0 a 9. Para isso, ele gera uma matriz de confusão, que mostra o quão bem o modelo está classificando cada dígito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 969,    1,    1,    0,    1,    2,    3,    1,    1,    1],\n",
       "       [   4, 1124,    2,    1,    0,    1,    1,    1,    1,    0],\n",
       "       [   5,    1, 1014,    3,    1,    0,    1,    5,    2,    0],\n",
       "       [   5,    0,    2,  982,    0,   10,    0,    6,    5,    0],\n",
       "       [   3,    0,    4,    0,  964,    0,    3,    2,    0,    6],\n",
       "       [   8,    0,    0,   11,    1,  862,    4,    1,    3,    2],\n",
       "       [   8,    3,    2,    1,    7,    5,  932,    0,    0,    0],\n",
       "       [  12,    3,    8,    1,    0,    0,    0, 1000,    0,    4],\n",
       "       [  15,    3,    2,    4,    4,    9,    5,    4,  927,    1],\n",
       "       [  13,    6,    0,    2,   15,    2,    1,   11,    2,  957]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste_matriz = [np.argmax(t) for t in y_teste] # Converte as probabilidades em valores de 0 a 9\n",
    "y_previsoes_matriz = [np.argmax(t) for t in previsoes] # Converte as probabilidades em valores de 0 a 9\n",
    "confusao = confusion_matrix(y_teste_matriz, y_previsoes_matriz) # Cria a matriz de confusão\n",
    "confusao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer previsões reais a partir do modelo criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Número a prever:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\")\n",
    "print(y_treinamento[0])\n",
    "print('Número a prever: ', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "O número previsto é: 5\n"
     ]
    }
   ],
   "source": [
    "novo = X_treinamento[0]  # Pega a primeira imagem da base de treinamento\n",
    "novo = np.expand_dims(novo, axis=0)  # Adiciona uma dimensão extra para a predição\n",
    "pred = modelo.predict(novo)  # Faz a predição\n",
    "pred = [np.argmax(pred) for p in pred]  # Converte as probabilidades em valores de 0 a 9\n",
    "\n",
    "pred = int(pred[0])  # Acessa o primeiro elemento da lista e converte para inteiro\n",
    "print(\"O número previsto é:\", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
