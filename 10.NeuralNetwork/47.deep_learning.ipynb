{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Profundo ou Deep Learning \n",
    "\n",
    "## O que é:\n",
    "Deep learning é uma subárea da inteligência artificial (IA) e do machine learning (aprendizado de máquina) que se concentra no uso de redes neurais artificiais profundas para resolver problemas complexos.\n",
    "\n",
    "## Como funciona:\n",
    "O deep learning utiliza redes neurais profundas para processar os dados. Essas redes consistem em neurônios artificiais organizados em camadas interconectadas.\n",
    "\n",
    "Cada camada recebe informações da anterior, transforma-as de acordo com parâmetros aprendidos durante o treinamento e passa os resultados adiante.\n",
    "\n",
    "O treinamento de uma rede neural é feito usando algoritmos de aprendizado, como o backpropagation (retropropagação), que ajusta os pesos das conexões entre os neurônios para minimizar os erros entre as previsões do modelo e os resultados reais. \n",
    "\n",
    "## Diferença entre machine learning e deep learning:\n",
    "→ Machine learning convencional geralmente depende da extração manual de recursos (informações sobre o problema). \n",
    "\n",
    "→ Deep learning elimina a necessidade de extrair manualmente características dos dados.\n",
    "\n",
    "## Diferença entre deep learning e redes neurais:\n",
    "→ Redes neurais são o conceito fundamental, representando um modelo matemático inspirado no \n",
    "funcionamento do cérebro humano.\n",
    "\n",
    "→ Deep learning é uma aplicação específica das redes neurais, que utiliza muitas camadas para realizar aprendizado profundo.\n",
    "\n",
    "⚠️ toda técnica de deep learning utiliza redes neurais, mas nem toda rede neural é usada em deep learning.\n",
    "\n",
    "## Tipos de algoritmos de deep learning:\n",
    "* Redes neurais convolucionais (CNN) → p/ imagens (reconhecimento facial, detecção de objetos e análise de imagens médicas)\n",
    "* Redes neurais recorrentes (RNN) → p/ palavras (séries temporais, texto ou áudio)\n",
    "* Redes adversárias generativas (GAN) → p/ deepfakes (imagens, vídeos, músicas ou textos)\n",
    "* Modelos de difusão → p/ geração (imagens, áudio e até design molecular)\n",
    "* Modelos transformadores → p/ ChatGPT e Google Gemini (sequências de dados, como texto, áudio e séries temporais)\n",
    "\n",
    "## Por que usar o deep learning?\n",
    "→ Para analisar dados não estruturados com eficiência: \n",
    "* Machine learning é comum trabalharmos com dados estruturados, tabelas com informações organizadas por colunas.\n",
    "* Deep learning se destaca por sua habilidade de lidar com dados não estruturados, como imagens, áudios, textos e vídeos, que representam a maior parte das informações disponíveis no mundo real.\n",
    "\n",
    "## Referências:\n",
    "* https://www.alura.com.br/artigos/deep-learning?srsltid=AfmBOooSLdvjpGMPscvEjYUV2DSGTaYsRP7lAXVjKxTWwcF0tL8dnmP1\n",
    "* https://didatica.tech/introducao-a-redes-neurais-e-deep-learning/#:~:text=Deep%20Learning%20x%20Redes%20Neurais&text=Conforme%20j%C3%A1%20mencionamos%2C%20o%20deep,n%C3%A3o%20estamos%20utilizando%20deep%20learning.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter os dados e já dividi-los em treinamento e teste automaticamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_treinamento, y_treinamento), (X_teste, y_teste) = mnist.load_data() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibir um item da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_treinamento[21], cmap='gray')\n",
    "plt.title(y_treinamento[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudar dimensão para 784 (original está em 28x28)\n",
    "Seguir as etapas de transformação de ↑→ para →, que são comuns no pré-processamento de dados para tarefas de aprendizado profundo, especialmente quando se trabalha com imagens. \n",
    "\n",
    "\n",
    "Na base de dados que está armazenada na variável `X_treinamento`\n",
    "\n",
    "Passo 1: Entender a distribuição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (60000, 28, 28)\n",
      "Qtd linhas:  60000\n",
      "Qtd colunas:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma: \", X_treinamento.shape)\n",
    "print(\"Qtd linhas: \", len(X_treinamento))\n",
    "print(\"Qtd colunas: \", X_treinamento.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Redimensionamento dos dados (reshape):\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "Em muitos modelos de aprendizado profundo, especialmente redes neurais densas (totalmente conectadas), a entrada precisa ser um vetor unidimensional. Imagens, por outro lado, são dados multidimensionais (largura, altura e canais de cor).\n",
    "\n",
    "O reshape transforma cada imagem em um longo vetor, \"achatando\" a matriz multidimensional em um vetor unidimensional. Isso torna os dados compatíveis com a camada de entrada da rede neural.\n",
    "\n",
    "np.prod(X_treinamento.shape[1:]) calcula o número total de elementos em todas as dimensões, exceto a primeira (que geralmente representa o número de amostras). \n",
    "\n",
    "Isso garante que o vetor resultante tenha o tamanho correto.Transformar de uma imagem quadrada ↑→ para uma imagem linha →"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento = X_treinamento.reshape((len(X_treinamento), np.prod(X_treinamento.shape[1:])))\n",
    "#X_treinamento.shape[1:] → Pega todas as dimensões depois da primeira. No caso (60000, 28, 28), isso retorna (28, 28).\n",
    "#np.prod((28, 28)) → Multiplica 28 × 28 = 784. Isso significa que cada imagem de 28×28 será achatada em um vetor de 784 elementos.\n",
    "#reshape → Isso transforma cada imagem 28×28 em um vetor unidimensional de 784 valores (achatando os dados), mantendo o mesmo número de amostras (60000).\n",
    "\n",
    "#Observação: Se estivéssemos lidando com imagens coloridas (RGB 28×28×3), o código adaptaria automaticamente para 28×28×3 = 2352 entradas.\n",
    "\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Conversão para float32 (para possibilitar a normalização [de 0 a 1] dos dados)\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "A normalização [de 0 a 1], que vem na próxima etapa, envolve dividir os valores dos pixels por 255. \n",
    "\n",
    "Essa divisão requer que os dados sejam do tipo float (número de ponto flutuante) para evitar arredondamentos indesejados.\n",
    "\n",
    "float32 é um tipo de dado de ponto flutuante de 32 bits, que oferece precisão suficiente para a maioria das aplicações de aprendizado profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento = X_treinamento.astype('float32')\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 4: Normalizar os dados (255 é o valor máximo de um pixel)\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "A normalização é uma técnica crucial para melhorar o desempenho e a estabilidade do treinamento de redes neurais.\n",
    "\n",
    "Dividir os valores dos pixels por 255 escala os dados para o intervalo de 0 a 1. \n",
    "\n",
    "Isso garante que todos os recursos (pixels) tenham uma escala semelhante, o que facilita o aprendizado da rede.\n",
    "\n",
    "A normalização ajuda a evitar que grandes valores de entrada dominem o processo de treinamento e pode acelerar a convergência do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento /= 255  #Divide por 255 para normalizar os valores entre 0 e 1\n",
    "X_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 5: Transformar as 10 classes de saída da variável `y_treinamento` para dummies variables (one-hot encoding):\n",
    "\n",
    "\n",
    "**Por que fazer isso?**\n",
    "\n",
    "Em tarefas de classificação multiclasse, como a classificação de dígitos (onde você tem 10 classes ), a variável de destino (rótulos) precisa ser representada de forma adequada para a rede neural.\n",
    "\n",
    "to_categorical realiza o one-hot encoding, que transforma cada rótulo em um vetor binário. Por exemplo, o rótulo \"3\" seria transformado em um vetor com 10 elementos, onde o quarto elemento é 1 e os demais são 0.\n",
    "\n",
    "* [0, 1, 2, **3**, 4, 5, 6, 7, 8 e 9]\n",
    "* [0, 0, 0, **1**, 0, 0, 0, 0, 0 e 0]\n",
    "\n",
    "Essa representação permite que a rede neural produza probabilidades para cada classe e facilita o cálculo da função de perda durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_treinamento = to_categorical(y_treinamento, 10)\n",
    "y_treinamento[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer o mesmo na base de dados que está armazenada na variável `X_teste`\n",
    "\n",
    "Passo 1: Entender a distribuição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (10000, 28, 28)\n",
      "Qtd linhas:  10000\n",
      "Qtd colunas:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma: \", X_teste.shape)\n",
    "print(\"Qtd linhas: \", len(X_teste))\n",
    "print(\"Qtd colunas: \", X_teste.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2:  Redimensionamento dos dados (reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 222, 254, 254, 254,\n",
       "       254, 241, 198, 198, 198, 198, 198, 198, 198, 198, 170,  52,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  67, 114,\n",
       "        72, 114, 163, 227, 254, 225, 254, 254, 254, 250, 229, 254, 254,\n",
       "       140,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  17,  66,  14,  67,  67,  67,  59,  21,\n",
       "       236, 254, 106,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  83, 253, 209,  18,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 133, 254, 187,   5,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   9, 205, 248,  58,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 126, 254, 182,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  75, 251,\n",
       "       240,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "       221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         3, 203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  38, 254, 254,  77,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  31, 224, 254, 115,   1,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 133, 254, 254,  52,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  61, 242, 254, 254,  52,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254, 219,  40,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
       "        18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Faz a mesma coisa com a base acima, porém com a partição de teste\n",
    "X_teste = X_teste.reshape((len(X_teste), np.prod(X_teste.shape[1:])))\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3: Conversão para float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  84., 185., 159., 151.,  60.,  36.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 222.,\n",
       "       254., 254., 254., 254., 241., 198., 198., 198., 198., 198., 198.,\n",
       "       198., 198., 170.,  52.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  67., 114.,  72., 114., 163., 227.,\n",
       "       254., 225., 254., 254., 254., 250., 229., 254., 254., 140.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  17.,  66.,  14.,  67.,  67.,  67.,\n",
       "        59.,  21., 236., 254., 106.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  83., 253., 209.,  18.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  22., 233., 255.,  83.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 129., 254., 238.,  44.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  59., 249., 254.,  62.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 133., 254., 187.,   5.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   9., 205., 248.,  58.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 126., 254., 182.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  75., 251., 240.,  57.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  19., 221., 254., 166.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         3., 203., 254., 219.,  35.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  38., 254., 254.,  77.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        31., 224., 254., 115.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0., 133., 254., 254.,  52.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        61., 242., 254., 254.,  52.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0., 121., 254., 254., 219.,  40.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., 121., 254., 207.,  18.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste = X_teste.astype('float32')\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 4: Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste /= 255  #Divide por 255 para normalizar os valores entre 0 e 1\n",
    "X_teste[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 5: Transformação das classes em dummies variables (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste = to_categorical(y_teste, 10)\n",
    "y_teste[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar a estrutura da rede neural:\n",
    "`Entrada (784)` → `Camada oculta (64)` → `Camada oculta (64)` → `Camada oculta (64)` → `Saída (10)`\n",
    "\n",
    "\n",
    "* 3 camadas ocultas com 64 neurônios cada e ativação ReLU.\n",
    "* Dropout (20%) é utilizado para zerar o um percentual de neurônios após cada camada oculta para evitar overfitting.\n",
    "* Camada de saída com 10 neurônios e ativação Softmax, ideal para classificação multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Armazena na variável o modelo Sequencial (feedforward), onde as camadas são adicionadas uma após a outra (em sequência)\n",
    "modelo = Sequential() \n",
    "\n",
    "modelo.add(Dense(units = 64, # Adiciona uma camada densa totalmente conectada com 64 neurônios\n",
    "                 activation = 'relu', # ReLU (Rectified Linear Unit 𝑓(𝑥)=max⁡(0,𝑥) melhora o aprendizado e evita saturação como o sigmoid (vanishing gradient)\n",
    "                 input_dim = 784)) # Entrada tem 784 neurônios (útil para imagens 28×28 achatadas)\n",
    "modelo.add(Dropout(0.2)) # Zera aleatoriamente 20% dos neurônios durante o treinamento evitando overfitting (forçando a rede a não depender excessivamente de neurônios específicos)\n",
    "\n",
    "modelo.add(Dense(units = 64, activation = 'relu')) # Outra camada densa com 64 neurônios e ativação ReLU\n",
    "modelo.add(Dropout(0.2)) # Dropout aplicado novamente para regularizar a rede\n",
    "\n",
    "modelo.add(Dense(units = 64, activation = 'relu')) # Igual\n",
    "modelo.add(Dropout(0.2)) # Igual\n",
    "\n",
    "modelo.add(Dense(units = 10, activation = 'softmax')) # Camada com 10 neurônios, pois temos 10 classes (dígitos de 0 a 9)\n",
    "# A função softmax converte os valores em probabilidades, garantindo que a soma seja 1, permitindo que o modelo escolha a classe (de 0 a 9) com maior probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo da arquitetura da rede neural gerado pelo comando modelo.summary():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,210</span> (231.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,210\u001b[0m (231.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicação:\n",
    "\n",
    "1. **Modelo:** sequential_1\n",
    "   * Modelos sequenciais são uma pilha linear de camadas, onde cada camada tem exatamente um tensor de **entrada** e um tensor de **saída**:  \n",
    "     * `Entrada (784)`** → `Densa (64)` → `Dropout (0.2)` → `Densa (64)` → `Dropout (0.2)` → `Densa (64)` → `Dropout (0.2)` → **`Saída (10)`**\n",
    "\n",
    "2. **Layer (type)** - Camadas: \n",
    "* **Dense:** `dense_4`, `dense_5`, `dense_6`, `dense_7`\n",
    "  * Essas são camadas densas (ou totalmente conectadas). Em uma camada densa, cada neurônio está conectado a todos os neurônios da camada anterior.\n",
    "  * Cada camada densa realiza uma transformação linear nos dados de entrada, seguida por uma função de ativação (que não é mostrada aqui, mas é crucial).\n",
    "  * As camadas densas são responsáveis por aprender padrões complexos nos dados.\n",
    "  * Como os parâmetros são calculados: Cada camada Densa (Dense) tem dois tipos de parâmetros treináveis\n",
    "    * Pesos (weights): Cada conexão entre os neurônios.\n",
    "    * Vieses (biases): Um valor adicional para cada neurônio.\n",
    "  * A fórmula para calcular o número total de parâmetros em uma camada densa é: \n",
    "    * $Parâmetros=(neurônios da camada anterior * neurônios da camada atual) + neurônios da camada atual$\n",
    "* **Dropout:** `dropout_3`, `dropout_4`, `dropout_5`\n",
    "  * O dropout é uma técnica de regularização que desativa aleatoriamente uma fração dos neurônios durante o treinamento.\n",
    "  * Isso ajuda a prevenir o overfitting (quando o modelo se ajusta demais aos dados de treinamento e não generaliza bem para novos dados).\n",
    "\n",
    "3. **Output Shape** - Formato de Saída\n",
    "  * `(None, 64)` significa que a saída de cada uma das primeiras camadas densas e dropout tem esse formato \n",
    "    * `None` representa o tamanho do lote (batch size), que pode variar.\n",
    "    * `64` indica que cada camada densa tem 64 neurônios.\n",
    "   * `(None, 10)` na camada de saída (`dense_7`) tem 10 neurônios, o que sugere que este modelo está sendo usado para uma tarefa de classificação com 10 classes.\n",
    "\n",
    "4. Param # - Parâmetros\n",
    "  * **Cálculo dos valores exibidos:** esses números representam a quantidade de parâmetros (pesos e bias) em cada camada densa\n",
    "    *  1️⃣ Primeira camada densa (`dense_4`): $(784×64)+64=50.240$\n",
    "    *  2️⃣ Segunda camada densa (`dense_5`): $(64×64)+64=4.160$\n",
    "    *  3️⃣ Terceira camada densa (`dense_6`): $(64×64)+64=4.160$\n",
    "    *  4️⃣ Camada de saída (`dense_7`): $(64×10)+10=650$\n",
    "    *  ✅ Total de parâmetros treináveis = $59.210$\n",
    "   * As camadas de dropout não possuem parametros treinaveis, dessa forma o parametro é $0$.\n",
    "   * Quanto mais parâmetros, mais complexo é o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar os parâmetros da rede neural e treiná-la utilizando a base de dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', # Otimizador Adam é uma variação do gradiente descendente que se adapta à taxa de aprendizado\n",
    "               loss='categorical_crossentropy', # Função de perda para classificação multiclasse\n",
    "               metrics=['accuracy']) # Métrica para avaliar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenar o histórico das execuções (erro e accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7569 - loss: 0.7520 - val_accuracy: 0.9515 - val_loss: 0.1647\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9318 - loss: 0.2363 - val_accuracy: 0.9608 - val_loss: 0.1312\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.1899 - val_accuracy: 0.9660 - val_loss: 0.1140\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.1655 - val_accuracy: 0.9669 - val_loss: 0.1158\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1489 - val_accuracy: 0.9711 - val_loss: 0.1014\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9598 - loss: 0.1368 - val_accuracy: 0.9705 - val_loss: 0.0989\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9610 - loss: 0.1324 - val_accuracy: 0.9722 - val_loss: 0.0913\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.1210 - val_accuracy: 0.9720 - val_loss: 0.0945\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.1137 - val_accuracy: 0.9732 - val_loss: 0.0937\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9674 - loss: 0.1087 - val_accuracy: 0.9719 - val_loss: 0.0925\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.1074 - val_accuracy: 0.9740 - val_loss: 0.0942\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.1043 - val_accuracy: 0.9730 - val_loss: 0.0903\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.0989 - val_accuracy: 0.9743 - val_loss: 0.0931\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9717 - loss: 0.0977 - val_accuracy: 0.9728 - val_loss: 0.0946\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0978 - val_accuracy: 0.9756 - val_loss: 0.0878\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0935 - val_accuracy: 0.9742 - val_loss: 0.0904\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9740 - loss: 0.0870 - val_accuracy: 0.9759 - val_loss: 0.0860\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0903 - val_accuracy: 0.9706 - val_loss: 0.1072\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9743 - loss: 0.0843 - val_accuracy: 0.9770 - val_loss: 0.0847\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9745 - loss: 0.0860 - val_accuracy: 0.9758 - val_loss: 0.0918\n"
     ]
    }
   ],
   "source": [
    "historico = modelo.fit(X_treinamento, y_treinamento, epochs = 20, validation_data = (X_teste, y_teste)) # Treina o modelo com 20 épocas e valida com a partição de teste\n",
    "\n",
    "# Esquerda  | Direita "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar Accuracy e Loss em um gráfico\n",
    "\n",
    "Interpretação: o erro (embaixo) foi caindo e a acurácia foi subindo (em cima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25b61a27b10>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAG0CAYAAAAsOB08AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARbFJREFUeJzt3Ql8VdW59/EnM3MAmWfEeQJlEucBRa3eorZF6xWkaluvelVerxUrUIfKdWptnVCrRTso1qrXOuBAtYqiKNRWK2JBJoEwQyBAEpLzfv5rZ4eTkIScTCdrn9+Xz2afs3OGtad1njXtnRaLxWIGAADgkfRkJwAAACBRBDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7KRfAPPfcc3bvvfdaaWlpspMCIMLIa4DGlVIBzPvvv28XX3yxHXrooZaenviq/+xnP7O0tLRGSZuPTjrpJDc1tn79+tkll1xivnjnnXfccaI5UhN5TcMir6n+uOhXyzRPnz7dvXfp0qWNlr5f/epX1rZtW/vWt75lq1evtlGjRtmLL77oVwDz0EMPuQ01fPhway42btxoF154od1///12xhlnWBRoG4eTMskePXrY6aefzg/nXk7gvU3KEJC47du3uwy2KY8/8pqmQV6TmLVr11pmZqb953/+Z7Wv2bp1q7Vs2dLOO+88i4qf//zndtNNN1lhYaH17NnTvvrqKzv11FMb7fsyG+ND//CHP7gfgblz59qiRYtsv/32s2T79NNP7fbbb7exY8fW+TNuvvlmu/HGG605Oe2009w66ZZWS5YscRn6KaecYq+88oqdeeaZyU5es3LCCSfY7373uwrLLrvsMhs2bJj98Ic/LF/Wpk2ben/Pjh07LDs721ItgLnlllvc46YoLQt5TdMhr6m9Ll26uO31f//3f+68aNWq1R6vef75523nzp01Bjm1sXDhwjrV8jWGOXPm2IABA2zixImWl5dn++yzj2VlZfkTwOjA/uCDD9zO+dGPfuQymClTplhTq3zQ6ESrL0XUmpqTAw44oMIJcO6559oRRxxh9913X70zlYKCAmvdurVFxb777uumeD/+8Y/dspoykV27drl+DLUNSJSZtGjRot7pRc3Ia5oWeU1iLrroIps5c6a99NJLdsEFF+zx9z/+8Y+Wm5vrmlvqIycnx5oLBS+hbt26Nfr3NXjYpkykQ4cObqd85zvfcc+rsnnzZrvuuutc6Uk7oFevXi66X79+fY3tdVX1L1Bp77DDDrN58+a50q8yE1VjiSJgpUVVnvoebeDbbrvNSkpK9kjTRx99ZGeddZZLv04mnZxq06up/fG3v/2ty7AUcevzDznkEHv44Ydrvb2+/PJLt506duzofvSGDBniDvi6Ovzww61Tp04uc0/kO8Lt/be//c3+67/+y62P9kno0UcfddtOVZ6qsXjvvff2+O6ioiKbPHmyDR482J2Y2obHH3+8vf3227VKu0p2Krnqe7UPTz75ZPvXv/5V7fFz7bXXWu/evd12V8n7zjvvrHeHSR1v2g733HOPy5i1zvr8L774otbbsqZjVJ+j9dL6qYr1rrvuqtM2jE/ngw8+6IIwfaaq9VesWOG2pY5zbUvts29/+9uuaaOy1157zX2+vidsu668zdW+rlqplStX2ujRo93jzp072/XXX19+Hik9WiaqhQmbG3TOhP7617+Wf1f79u1dmhYsWFDnfUVeQ17TnPMaBXhKlwKVqpqYZs2a5baVPlPr+N3vftf69Onjnuu7dMyqJndv+lXRB0bromNF21DrqHWtKr0Necyq5lHnVf/+/d2+VwDzgx/8wDZs2LDHZ/397393QW+7du1cfqJmpg8//NAS1eAhvjIRtemptKp2YJ1gH3/8sQ0dOrT8Ndu2bXMHmzIvreBRRx3lMhMd6N988407KRKljaQNokhXpYSuXbuWnyza2BMmTHBzHTQ68PPz8+3uu+8uf/+bb75pZ599tnXv3t2uueYat/GVvpdfftk9r47WTx31/uM//sOVmP7yl7+4k1IHy5VXXlljmnWQHXvsse6HTNXFSt+zzz7rfiT+/Oc/uxMgUZs2bXJTWJWe6Hco7foh0jZSqUgef/xxV8I95phj3In89ddfu/VVJqUTLaRt+pvf/Mbt98svv9y18eq96silKv5BgwbVmHZ9p040nSSa5s+f736QlVlVLvGeeOKJ7gdV6dJJr5K4qi3VcUyBR33px0LVu2pa0omtda3v/tJ+UZ8InR/f+9733CiVn/zkJ+6HICzBJroNdb5p+1x99dUuQFFApM9W5qUfXn2+mlbUH0MBxxNPPFH+XjWnjRs3zn22MmRtVx3Pxx13nMtg4vsCKUPT69TXREHTW2+95UbYKMO74oor3DGj9+qxtkPYrq9MTvR6raMCLf04K2NWmrQ9tZ/r0u+IvIa8pjnnNVp/Bek6z3Vuah1CM2bMcOeUamnkT3/6k/sunT9qdtE66PzQMaq/JSIvL88FZKo5DveDgkIFM5XpmFUAoWNWcxUy6nrMvv76664QoPNMf9fxoO/VXMFJGJDruc5JBS833HCDa2J65JFHXOFAQW1C/dliDeiTTz6J6SPffPNN97y0tDTWq1ev2DXXXFPhdZMnT3ave/755/f4DL1Hfvvb37rXLFmypMLf3377bbdc89CJJ57olk2bNm2Pz9u2bdseyy677LJYq1atYjt37nTPd+3aFevfv3+sb9++sU2bNlWZHpkyZYr7nnjbt2/f4/NHjRoV23fffWN7c+qpp8YOP/zw8nSE33fMMcfE9t9//72+X2m59NJLY+vWrYutXbs29tFHH7nP1PJ77703oe8It/dxxx3ntkeoqKgo1qVLl9igQYNihYWF5csfffRR93pt+5DeF/8a0fbs2rVr7Ac/+EGN66L0Z2dnx771rW9V2OY33XST+55x48aVL7vttttirVu3jn311VcVPuPGG2+MZWRkxJYvXx6rLX1O/GfreNP3tWvXzqUpXm23ZU3H6FNPPVW+TNuqW7dusfPPPz/hbRims3PnzrHNmzeXL584caJbPnDgwFhxcXH58gsvvNBt3zDtW7dujbVv3z52+eWXV/iuvLy8WG5uboXl2j76zFtvvbXCa4888sjY4MGDy5/rONTrdJ5UpuNHx9GGDRvKl/3jH/+Ipaenx8aOHRtLFHlNgLymeec1r7zyivvMRx55pMLyo48+OtazZ89YSUlJtft26tSpsbS0tNiyZctqPC769u1bIc3XXnute432Ufw667yufJxX9b0/+tGP6nTMFhQU7PFZTz/9tPvOd999t3zZ6NGj3fZfvHhx+bJVq1bF2rZtGzvhhBNiiWjQJiSViFQaUfQnirjGjBljzzzzTIUqKUXjAwcOrDLqr+vQQZWSx48fv8fy+HZVpUGlapWCFe2qulNU2lQ1qCJ+VW0nkp74qHbLli2udKeIXSUHPa+OInJFuyotq/Sg92lS6U6liH//+98u6t8blTpUilE1rCJXDd9UNK11qct3qDSTkZFR/vyTTz5x1Z3qKxLfB0RVlqq6jaf3ha9RqVDfr1KAqpFVwqmJSuhhTUL8Ntd6VKYSiSJ4VWWG66Rp5MiRbh+/++67Vl/nn39+eZNIQ+0vlXDi+xBoW6mKXMdKXbehqp3j90NYetH3xPeh0HJt3zCNKlGpalwl2PhtqO/Xa6uqitcxEE/7ID7t1VFJVdXLOmbiS6GqnVFHx1dffdUSRV5DXuNDXqNaHW2z+GYk7X/VSOjcCzvfxu9b1UbpO1QLpdhRx0wiXn31VTv66KNd3hJSGsLannjx3xvuN61vXY7Z+H5gOvb1WUqHhPtE2+yNN95wtXLx/RFVs/P973/fZs+e7Wp/mrwJSQlT5qEMJb5NVAe6qppVnaqdKYsXL3Y/EA1J1ZZVdbLUMC61yStDXrNmTYV2wPCkV3pEbduJ0kmsjoPqfa2dHk+fX/nEC6laXwfnpEmT3FQVncxar5qoivKqq65yB5L6MKiKOcxI6/Idar+Mt2zZMjfff//9KyxXtV/lDrHy5JNPuv2tg7+4uLjaz62suu/RiafMI54yw3/+858VAozK61RfldPbEPtLbdGVf6S0blqXum5DVWnHC4+3+Or2+OWq8g+3YU0dTlW9G09t2pW3t9Iefl5t9u2BBx64x98OPvhgV/WcSCdO8hryGl/yGhUiFFhrxJYCOK1/GMzEBxTLly93TTdq2qx8TtUUnFa3flU1w1R1/qk5RyPeFHxWDhwSPWYVROr417lZebuEn7Vu3Tp37FaXF+icUR8+HVtNGsBoA6ikpcRrqqrEFGYqtVFdaaSqzkVSVfuedoiiSZ3Yt956q2urVUas9kW129W3w6d2rDofHXTQQfaLX/zC/WgoY1ME/Mtf/rLGzw//pn4JKqFUpTZDQvWjqNJAQ31HVduxtn7/+9+70pKi6//5n/9xJTWVlKZOnVp+EjQErZdK7mo/rW60RH1V3g4Nsb/iS5vxghr6um3D6j5zb98Vro/6wVQ1WqDyCJjqPi8ZyGvIa3zKa1Qb+sADD9jTTz/tto/m6oAd9tPRcabvUACgPmvaxwoMFfBoHRvrSs6bN292NXgqrOiYVX82HbOqLVE6Ev1e1b6pf5D2h9ZNNc76DNVCNtY6NFgAo0xDB5FGRFSmYY4vvPCCTZs2zR202lCff/55jZ8XRsLayFVFz7WhkpAiQX2/OpeFKpd4w6FfSlN1J2hV1IlOF+xR1BxfEq5NT/iwRKHSRSLfmYiG+I6+ffuWl0TiS+sq8aj0q+r5kDqr6Tu1veN/FGoztDX+e+JLW4rYK5dItL/UObOxtluy9ld9t2EiwmNe52xDrU91gUC4b3W9ispUelZH2kSG0JLXkNf4lNeoNkSfo5oXBSqq9dAF30KfffaZq71TjVL8tYPUzFsXffv2La9hjVf5/FMnfzXxaRtqRF0ovlaztsestptqPlUDo5qkUOV0qCZLTU3V5QVqUqtce1yTBukDoxEF2gjqpaxhYZUnVTuqfS0cTqcq3X/84x8uo6muhBhutPg2RkWq6tVcW+GBHV+9qExA0XA8jUxQtaN6lFfOxOJLx5WFpdL416iqTCNY9kYZsHpdq/e1SpOV6WSqr4b4DrUp66DTD0J8D331Xq+8raraHhp6pyrvvdGJocxPPe/j319VL39F+vpMNT1UpjSpLbyhNcX+qu82TIRKySp53XHHHRXOj/qsT9gGXvm4UPu2SmTKoOP/pgxR7eEaBVJb5DXkNT7mNWouUl8SBVg6VtTfo6Z10eP4IcqJOOuss1wfG9X+xW//ypcZqOp7td3V3JXoMVvVZ1W1TfU61Y5q+Hb8ZQvU5KoATyMgKzdfN3oNjDILZRoa7lYVdeTRgakNqPZAVTEpglYHRA250lh+VZ/pc3TwKtJWG5jep+Fq4RA0VRcn8uOkTlDqdKRquP/+7/92B85TTz21R/W4oj4NUTznnHNcRqsOesp0FREqWq7q4BXtCFXj6n0aYqdI/bHHHnMnc1UncWUqQWqHaRitOrSpNKAdqRNGw+eU8dZXfb9DJ7qGG2r9VCrS/lOEroyzcru0flT046IOk7q2gF6n/anqUm2bmoTXFVEVsD5HJ6FOeF2npPJQVx0/Olb0Ou1bHT/qQ6GSjI4rnRh1GR7bHPZXfbZhIpRJ6JjX/XqUQWlIsPaB2uJ1ZVXVIlT+8d0b1XgonRoiqqp1nbNqN9ekIZkaejxixAi79NJLy4dRq8kl/loxe0NeQ17jY16jZiQ10+iHW+dW/GUD1GSkIFppUrORzk11Pq9N/7KqqLlLTcNqulHzZTiMWjUz8TWCOmZV+6hLKYTHrN5XOQipzTGrNKsWR5dxUBCvvj4qnFSuzRHtY9Uu6VjRUHqdIwp8FfBXvi7WXsUawDnnnBNr0aJFlcOoQpdcckksKysrtn79evdcwymvuuoqN5RMQ6o0BFJDwcK/i4ZZjRw5MpaTk+OGx2mYm4ZNVjW08dBDD63ye997773Y8OHDYy1btnTfpc9444039vgMmT17duy0005zw7k0dO6II46I3X///TUOYXvppZfc67T+/fr1i915552xJ554osphmVXROmoYqYbTavsojWeffXbsueee2+t79R1XXnllg3xHOLTx448/rvIzHnroITeUTvtiyJAhblictnv80EYNqbvjjjvccDu9TsNsX375ZbdftWxvNKTwlltuiXXv3t3tr5NOOin2+eef7zFMMBwGrCHD++23nzt+OnXq5IZr3nPPPW44Zn2HUd9999113pbVDb+t6hitvG1quw2rS2f43X/6058qLK9u/+r1GoqrIZY6hgcMGODOVQ1Tjk+jtlNlVZ0PH3zwgRtarX1SeUj1W2+9FTv22GPdvtUwdeUbX3zxRSwR5DXkNb7mNUOHDnXrrfWrTOeBjr82bdq4z9dlDHSZAb1e2yuRYdTyz3/+020vHSvaBxoO/vjjj+9xrLz//vtuSLe2QY8ePWI33HBD7PXXX6/xmNWlD/T3ysfsN998Ezv33HPd5RmUn3z3u991w6OrurTC/PnzXb6j9dWQ7ZNPPtnlHYlK03+JhTwAACAVlZaWulpV1RJp5FAyNY87QAEAgGYvPT3d9aHTaKpka153CwMAAM3SI4884jri6iaVzeEO5NTAAACAvdJ1XnTfLQUxla/MnQz0gQEAAN6hBgYAAHiHAAYAAHiHAAYAAHgn05dx56tWrXJ3QN3bLecBNDx1ldMVcHv06OGGUfqAfAOIdt7hRQCjTCiRGzwBaBy61b3uSuwD8g0g2nmHFwGMSlDhBkjkRk8AGkZ+fr4LBsJz0QfkG0C0846EAxjdsVU3Zps3b567iZju8jp69Oga36Pbdk+YMMHd+EkrcvPNN7sbY9VWWP2rTIiMCEgen5piyDeAaOcdCTdI6U6cuoOr7jxaG7obpe4WevLJJ9unn35q1157rV122WXV3nUVQPSo4KO72aodXBnZiy++uNf3qOCjO2Xn5OTYfvvtZ9OnT2+StALwQ8I1MLp8cCKXENYtzvv372/33nuve66bP82ePdt++ctfuvspAIi+sODzgx/8wM4777xaF3x0tc8//OEPNmvWLFfw6d69O/kGgKbpAzNnzhwbOXJkhWXKgFQTU53CwkI3xbehAfAXBR8ADa3Rx0Pm5eVZ165dKyzTcwUlO3bsqPI9U6dOtdzc3PKJkQRAaqmu4KPlACDN8oIOEydOtC1btpRPGkUAIHXUpeCjWlv9PX4CEF2NHsB069bN1qxZU2GZnmtUQMuWLat8jzrthSMHGEEAoDaouQVSS6MHMCNGjHAd8OK9+eabbjkANFTBh5pbILUk3Il327ZttmjRogqjBTQ8umPHjtanTx+XiaxcudKeeuop93eNInjggQfshhtucCMQ/vrXv9qzzz5rr7zySsOuCYDIUAHn1VdfTajgo5pbTQBSQ8I1MJ988okdeeSRbhJdoE6PJ0+e7J7r4nbLly8vf71GEihYUeajYZQaVfCb3/yGkQRAClHBRwUdTfEFnzCvUMFn7Nix5a9Xwefrr792BZ8vv/zSHnroIVfwue6665K2DgCal7SY7rTUzKkzntq0VS1MfxjAv3NQF6XTxSwrGzdunLtAna7MvXTpUve6+PcoYPniiy/cPVQmTZqU0BW8yTeA5GvM85AABkAkz0Ef0wxETX4jnofNchg1AACA93ejBnykys3CXaW2rXCXbd25y7bt3GVbC4vdPFxWULTL0tPSLDM9mDIy0i1LjzPSg2UZWh48zkhPc59XXFJqRbtKrahk9+MKy3eVWmnMyt5bNmWku/dnZehz0svmaZalz85Is/OOatjb3AMI6HzU+dYQNzMsLY3ZtqJdlr+j2LIz061zm5wG+dy8LTtt/vJN9tnKLVYai1mrrExrlZ1hLTVlZZQ/bpWd6Z4HjzMst2WWtc5JXhhBAINI2VFUYlt2FLsfc027SmPBvCRmu0q1LOYeF5cGy+J/9BUQxM/jgwMXLFQKGopKYla0q8R9ZuXXby8KghT9rbnLzkgngEGDBu47i4NzYEdxiQvAW+VkWKusDBdIJ2JXSak7nzdr2l5kmwqKbdP2Iiso3GXZmRnWIivdcsrmLbLin+9epgKC0rK9qCSYClVwKClfps9SvqFl6WnmfpDbtsi01tmZ1qZFprXJyXTL2oRTi0y3LlsLd9m6rTttbX6hrd2qKXi8Ro/zd9o6t6zQ5QOKMfR5rXMyyuZBgKDPa+U+NwgOsjLSbVthsSvcBFOx5e8I5q4QVLTL4jt97NM62w7u3s4O6dHODu7e1g7pnmv7dm7tPqc6yqO+WJ1v85dtsnnLN9nfl22yVVt21mlf/+jEfW3imQdbshDAoN5KSpVhlbjMSnNlXoW7Snb/+FeYV1wuqgkIJ9UWpIe1EW6umoPgVuw6gV0m5qaKGdrm7cFcn9scxWd+yhz1WHNlWsqQFFwp2NpVHmztDrhKypaXxGIu2FDJSxlUTmbwWMv03D0um5RpB++LVfzssmX6mwIuzbW90fzpB3fJ+gJbsXF7UMPmatSCfa9atOBxcM5kZwZzHQdBbV+x5Zf9IG6Nm+eXL99lO4tK3A+t3pOuc07/wudlc52HWqbjxgUoRcF5r0BAj928uKTaddCx2bqsJK8f8GAKfthzsjJc7aTOawUsmwqKXNqiQOe49oMms933+asrbcddJaW2oaDIZi9a76byv2Wk2/5d29gh3RXUBJOCQNWwKGhRLUvlfFL796Bu7WxQn/auhkX7cEdckBfu53Afa7nyedXUJBMBDFyJSaWPDdsKbb2bitx8w7aismXKUIrKDuLSIFgJD+jiICBpTsKmEjWPZGUGzS9hJl/xcXpcQJBW9uOfUb4su3yZHmdYVmZa3N92Bw1hMLH7edruEpuClexMggTUigJ8BShL1mvaFjcvsDX59f/ha2o6HxTsaJKwtlMFkEQo2O/QKtvat8qy9q2yrW1OpvsR1vZSfqTHYeEpmJfYzrhCks7PMFhyc9WqxAVQYROJAg3VyITBRvi4oLCkfFm4LmG6urTNsS5tW1iXdjkVHncue6xaEtX46jP0eW4qCj4zeBzOd1nxrpjLM9qVFXTatciytm4KnoePVbO0s7jEFuZttQWr892kWpUFq7e6NP5rVb6bqtOhVZYd2aeDDe7bwY7s094G9mpfp6agZI8BIoCJIJ1girg3FgS1FW5eUGQbXY2FnutvhbZxe7Gt31poGwoK3YnfEMIq3LB2IJgHz3cv2/13/ayrVkC1CyVlNQ8lpapt0HoEtREuA4zFXECgTKxDWSameYfWytSyrWNZ5qbnypgaol0YqAsdr0s3FLgfFQUd+pEtdLWSwQ+um5fVUgbLgh/d1Vt22MpNO1ztSnV0zPfdp7ULoPWjGDaPqmnT1bapGTNsLi3rC6UfpnaVfgCDH8bdz8trAy3m3qMfJv02qT9EVc9VEKjcP8L1i8jKLH+sv6kwofcqfdsLS2y7amsKy5puympwXHNOoUr0JS6t4bndPgxYWmYl3PQU32dEaa7r+2vq16b8SIFErbW1BtUiK8MG9m7vpvj0fbNphwtewqBGQY720VF9O9hRfTS1t/6dWjdIHpnsfJYAxgPKlDaWBSIbVTvi5oXuuaoQ4+eaVAVbUyZYHWU4ndpm2z6tc6xTG03Zbr5Pm2zr2Dq7vPOW5kEb8+7nmhSUUNOAVLJle7EtyMu3L8tKv1/m5dvCNVvrVSBQAN6/c2vrt09r27dTa+vXqbX7wdGkH3Xf6EcuKLRkWIcm/m7lR0ExqeHWJcz7mqO0tDTr3bGVm844rJtFHQFMEqhUoM5dKnGphkRNNW5eULGGRNWsasKpazuwSlUKPFRroWpM1U6Ezzu2zgqWt8lxPdkVpCSzNzngQ0HioyUb7YPF6+3L1UHVfXWdH1UTeWC3dta7Q8sguI+rmQw7nbrHWbtrI1VYUAfMhhpZAkQdv1iNRNWhastevnG7LdsQzMNJyxPtbKr8LAg8gkkBSVAzkuMeh8s6ltWWtG+Z7TJFAHWnPhTvL1pvr32+2t78Yk2VfTd6tm9Z1lmyrZsf1K2ta+ZR8wmAxkMA0wA0ZO6TZZvsk6Wb7POVW2zZxr13uFPm1q1di/KApLympM3uGhIFJ2FNiaqOyRCBpil8/O2rdfbaZ6tt1oK1brhsSH0zTjmoqw3snetGbRzUva3rTwKg6RHA1KH5Z/G6bfbx0k32ybKNLmhRrUpV1FO+zz6trE/HVuXzvh1bu3mP9i0arGMZgPpRp8y3v1xrMz/Ps7cXrnWdTEMaWTLq0G525mHdbFj/jpy3QDNBAFOLgEXj5+cu3WjzXNCyyY3wqdy8o9LY0H7BkLT+ndpY346tXA962rKB5u3u17+0x95bUuFyAGoWUidIBS0auUHndKD5IYCpoUT2p09W2JMfLLWlGyrWsKhT3qDe7V3AMrhfRxe0UI0M+ElNtApe+u3Tys48vLsLWg7vmUvhA2jmCGAqWbahwKZ/sNT+9Mk3ZVdNDEbzHLdfJ3fRn6H9OrrLNtd0qWYA/jj3yJ527H6dXOdbghbAHwQwZRf/+WDxBvvt+0ts1pdry+81oSGN44/p5+4TwxBjIJp0KQFNAPyS0r/Kugrki5+utOnvL3UXnwqdeEBnG39sPzth/860fQMA0AxlpmrH3Pv/ush++8ESdxNA0aWWzz+ql407pp/t16VNspMIAABqkJIBzP/9Y6X98q2v3ONeHVrauBH97HtDe1tuSzriAgDgg5QMYJ6eu8LNLz2uv9101sFcIA4AAM+k3FCar9dts7lLNppilsuO70/wAgCAh1IugJnxSVD7ctKBXax7bstkJwcAANRBSgUwxSWl9ud537jHY4b2TnZyAABAHaVUAKMbs63fVmSd2+bYKQd1SXZyAABAHaVUAPPMx8vd/DuDe3ElXQAAPJYyv+KrNu+wv321zj3+3hCajwAA8FnKBDC6t5FuEXD0vh2tf6fWyU4OAACoh5QIYEpKY/Zs2eijC4b2SXZyAABAPaVEAPP+ovW2cvMOa9ci0844rFuykwMAAOopJQKYGR8HtS/nHtnTWmRlJDs5AACgniIfwGzYVmhvfJHnHl8wjOYjAACiIPIBzPPzV1pxScwG9sq1g7u3S3ZyAABAA4h0ABOLxcqv/TKGzrsAAERGpAOYecs22eJ1BdYyK8POGdg92ckBAAANJNIBzDNlnXfPPqK7tW2RlezkAACABhLZACZ/Z7G98s/V7vEFw7jyLgAAURLZAOalT1fZjuIS279LGzuqT4dkJwcAADSg9Khf+2XM0N6WlpaW7OQAAIAGFMkA5vOVW+yzlVssKyPNzjuqV7KTAwAAGlgkA5jwvkenH9rNOrbOTnZyAABAA4tcALOzuMRe+PtK9/iCoXTeBQAgiiIXwLz2+WrbunOX9Wzf0o4d0CnZyQEAAI0gcgHMM3N3d95NT6fzLgAAURSpAObrddvsoyUbTXHLd4fQeRcAgKiKVAAzo6zz7kkHdrHuuS2TnRwAANBIIhPAFJeU2p/nfVPefAQAAKIrMgHMwrytVlBYYp3a5NgpB3VJdnIAAEAjyrSIOKxnrs396anu7tNZGZGJywAAQBUi9UuvO04P6t0+2ckAAACNLFIBDAAASA0EMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAwDsEMAAAIDUCmAcffND69etnLVq0sOHDh9vcuXNrfP19991nBx54oLVs2dJ69+5t1113ne3cubOuaQYAACku4QBmxowZNmHCBJsyZYrNnz/fBg4caKNGjbK1a9dW+fo//vGPduONN7rXL1iwwB5//HH3GTfddFNDpB+AJyj4AEhqAPOLX/zCLr/8chs/frwdcsghNm3aNGvVqpU98cQTVb7+gw8+sGOPPda+//3vu8zr9NNPtwsvvHCvmReA6KDgAyCpAUxRUZHNmzfPRo4cufsD0tPd8zlz5lT5nmOOOca9JwxYvv76a3v11VftrLPOqvZ7CgsLLT8/v8IEwF8UfAAkNYBZv369lZSUWNeuXSss1/O8vLwq36MM6NZbb7XjjjvOsrKybMCAAXbSSSfVWJKaOnWq5ebmlk+qPgbgJwo+ALwchfTOO+/YHXfcYQ899JCrOn7++eftlVdesdtuu63a90ycONG2bNlSPq1YsaKxkwmgkVDwAZD0AKZTp06WkZFha9asqbBcz7t161bleyZNmmQXX3yxXXbZZXb44Yfbueee6wIaZTalpaVVvicnJ8fatWtXYQKQOij4ANibTEtAdna2DR482GbNmmWjR492yxSE6PlVV11V5Xu2b9/uqovjKQiSWCyWyNcD8FB9Cz6iwk9BQYH98Ic/tJ/+9Kd75ClhwUcTgNSQcBOSRhI89thj9uSTT7rRAVdccYXLWNQ5T8aOHetKQqFzzjnHHn74YXvmmWdsyZIl9uabb7rMScvDQAZAdMUXfEJhwWfEiBFVvoeCD4AGrYGRMWPG2Lp162zy5Mmu/XrQoEE2c+bM8vbt5cuXV8h4br75ZktLS3PzlStXWufOnV3w8vOf/zzRrwbgKRV8xo0bZ0OGDLFhw4a5a7xULvj07NnTNS2L8giNXDryyCPdNWMWLVpEwQdABWkxD4ozGk2gTnlq16Y/DODnOfjAAw/Y3XffXV7w+fWvf+2CE1EHXQ2Xnj59unu+a9cuV8j53e9+t0fBp3379k2WZgD105jnIQEMgEiegz6mGYia/EY8D7mZIwAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAA8A4BDAAASI0A5sEHH7R+/fpZixYtbPjw4TZ37twaX79582a78sorrXv37paTk2MHHHCAvfrqq3VNMwAPkW8AaEiZib5hxowZNmHCBJs2bZrLhO677z4bNWqULVy40Lp06bLH64uKiuy0005zf3vuueesZ8+etmzZMmvfvn1DrQOAZo58A0BDS4vFYrFE3qDMZ+jQofbAAw+456Wlpda7d2+7+uqr7cYbb9zj9cqw7r77bvvyyy8tKyurTonMz8+33Nxc27Jli7Vr165OnwGg7up7DpJvAKkpvxHPw4SakFQqmjdvno0cOXL3B6Snu+dz5syp8j0vvfSSjRgxwlUFd+3a1Q477DC74447rKSkpNrvKSwsdCsdPwHwE/kGgMaQUACzfv16l4EoQ4mn53l5eVW+5+uvv3ZVwHqf2q8nTZpk9957r91+++3Vfs/UqVNdxBZOKqkB8BP5BgAvRyGpqljt2I8++qgNHjzYxowZYz/96U9dFXF1Jk6c6KqbwmnFihWNnUwAzQj5BoAG7cTbqVMny8jIsDVr1lRYrufdunWr8j0aQaA2bL0vdPDBB7uSl6qWs7Oz93iPRhxoAuA/8g0ASa+BUaah0tCsWbMqlJT0XO3VVTn22GNt0aJF7nWhr776ymVQVWVCAKKFfANAs2hC0lDIxx57zJ588klbsGCBXXHFFVZQUGDjx493fx87dqyryg3p7xs3brRrrrnGZUCvvPKK64ynznkAUgP5BoCkXwdGbdHr1q2zyZMnu+rcQYMG2cyZM8s76C1fvtyNMAipI93rr79u1113nR1xxBHueg7KlH7yk5807JoAaLbINwAk/TowycD1HIDk8vEc9DHNQNTkN5frwAAAADQHBDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA7BDAAAMA70QlgineaLf/QbMHLyU4JAABoZJkWFdvXmz0xyiw9y+zmNWbpGclOEQAAaCTRqYFp290sPdOstNhsa16yUwMAABpRdAIY1bjk9goeb16e7NQAAIBGFJ0ARtr3CeYEMAAARBoBDAAA8E7EApi+wXzzsmSnBAAANKKIBTDUwAAAkAoIYAAAgHeiGcBs+castCTZqQEAAI0kWgEM14IBACAlRCuA4VowAACkhGgFMEI/GAAAIo8ABgAAeCeCAQzXggEAIOoiGMBQAwMAQNQRwAAAAO9EN4DhWjAAAERW9AIYrgUDNFsPPvig9evXz1q0aGHDhw+3uXPn1up9zzzzjKWlpdno0aMbPY0AIhzANOtMiGvBAM3SjBkzbMKECTZlyhSbP3++DRw40EaNGmVr166t8X1Lly6166+/3o4//vgmSyuACAYwXmRCub2DOQEM0Gz84he/sMsvv9zGjx9vhxxyiE2bNs1atWplTzzxRLXvKSkpsYsuushuueUW23fffZs0vQAiFsB4kQmVD6UmgAGag6KiIps3b56NHDmyfFl6erp7PmfOnGrfd+utt1qXLl3s0ksvbaKUAohkANNUmVBhYaHl5+dXmOo2EolrwQDNwfr1611BpmvXrhWW63leXtV91WbPnm2PP/64PfbYY02TbwCIbgDTFJmQTJ061XJzc8un3r3LmoRqi6HUgNe2bt1qF198scs3OnXq1DT5BgCvpDe3TEgmTpxoW7ZsKZ9WrFiR2BcTwADNis7/jIwMW7NmTYXlet6tW7c9Xr948WLXb+6cc86xzMxMNz311FP20ksvucf6e4PnGwC8ktlUmVCotLQ0+OLMTFu4cKENGDBgj/fl5OS4qcGuBaORSQCSJjs72wYPHmyzZs0qH4WovEDPr7rqqj1ef9BBB9lnn31WYdnNN9/sCkW/+tWvqqxdqXe+ASC6AUxTZEKNci2Y3J6N8z0Aak2jF8eNG2dDhgyxYcOG2X333WcFBQVuQICMHTvWevbs6ZqCdImGww47rML727dv7+aVlwNITQkFMN5kQhmZZu16Bp141YxEAAMk3ZgxY2zdunU2efJk12du0KBBNnPmzPI+dcuXL3eDAgCgUQIYbzIhNSOFAUzfEclODQAzV1NbVW2tvPPOOzW+d/r06Y2UKgApEcB4kwm5a8G8R0deAAAiqBlUlTQSrgUDAEBkpUAAQw0MAABRQwADAAC8E/0AJrwWDAAAiIzoBjCVrwUDAAAiI7oBTHgtGKEZCQCASIluAFOhGYl7ogAAECURD2B0LRiGUgMAEDURD2AYiQQAQBQRwAAAAO8QwAAAAO+kSACzwqy0NNmpAQAADSTaAUz8tWC2cS0YAACiItoBDNeCAQAgkqIdwAj9YAAAiJwUCGC4FgwAAFGTAgEMNTAAAEQNAQwAAPAOAQwAAPBOCgUwXAsGAICoiH4Aw7VgAACInOgHMFwLBgCAyIl+ACP0gwEAIFJSJIDhWjAAAERJigQw1MAAABAlBDAAAMA7BDAAAMA7KRbAcC0YAACiIDUCGK4FAwBApKRGAOOuBdMjeEwzEgAA3kuNAKbCUGoCGAAAfJdCAUzYD4ZrwQAA4LsUDGCogQEAwHcEMAAAwDsEMAAAwDspGMBwLRgAAHyXOgFM2x5maRlcCwYAgAhInQBG14LJ7Rk8phkJAACvpU4AI1wLBgCASEixAIZrwQAAEAUpGsBQAwMAgM8IYAAAgHcIYAAAgHdSNIDhWjAAAPgstQIYrgUDAEAkpFYAw7VgAACIhNQKYIRrwQAA4L0UDGC4FgwAAL5L4QCGGhgAAHxFAAMAALxDAAMAALyTugHMlm+4FgwAAJ5KvQAmvBZMSZHZtjXJTg0AAKiD1AtguBYMAADeS70ARrgWDAAAXkvRAIZrwQAA4LMUD2CogQEAwEepHcBsWJTslAAAgDpIzQCmx1Fmlma27H2zBS8nOzUAACBBqRnAdDnI7Nj/Dh6/dLVZ/upkpwgAACQgNQMYOflms+4DzXZsNHvxx1zUDgAAj6RuAJOZbXb+42aZLc2+fsfsw4eSnSIAAFBLqRvASKf9zc6YGjyedYvZ6n8mO0UAAKAWUjuAkcGXmB10dnBrgT9fZla0PdkpAgAAjRHAPPjgg9avXz9r0aKFDR8+3ObOnVvtax977DE7/vjjrUOHDm4aOXJkja9vcmlpZuf82qxNN7P1C83enJTsFAGRFam8A4BfAcyMGTNswoQJNmXKFJs/f74NHDjQRo0aZWvXrq3y9e+8845deOGF9vbbb9ucOXOsd+/edvrpp9vKlSut2Wi9j9m5DwePP/6N2cLXkp0iIHIimXcASJq0WCwWS+QNKjUNHTrUHnjgAfe8tLTUZSxXX3213XjjjXt9f0lJiStN6f1jx46t1Xfm5+dbbm6ubdmyxdq1a2eN5vWfms15wKzVPmZXzDFr27XxvgvwSEOcg02ddzRZvgEgKedhQjUwRUVFNm/ePFeVW/4B6enuuUpItbF9+3YrLi62jh07WrNz6mSzroebbd9g9uIVDK0GGkhT5B2FhYUus4yfAERXQgHM+vXrXSmoa9eKNRN6npeXV6vP+MlPfmI9evSokJE1m4woM8fs/N+YZbYwWzzLbO4jTfO9QMQ1Rd4xdepUV9ILJ9XuAIiuJh2F9L//+7/2zDPP2AsvvOA68VUnqRmRrtJ7+u3B4zcnm+V93nTfDaDOecfEiRNdNXU4rVixosnTCaCZBjCdOnWyjIwMW7NmTYXlet6tW7ca33vPPfe4TOiNN96wI444osbXJj0jGnqZ2QFnBEOrn7/crHhH034/EDFNkXfk5OS4Nvb4CUB0JRTAZGdn2+DBg23WrFnly9QRT89HjBhR7fvuuusuu+2222zmzJk2ZMiQvX5P0jMiDa3+jwfMWncxW/uF2ZtTmvb7gYhpqrwDQOpIuAlJwyB1fYYnn3zSFixYYFdccYUVFBTY+PHj3d81OkA1KKE777zTJk2aZE888YS7/oPauzVt27bNmrU2nc1Glw2tVl+Yf72Y7BQBXkuZvANAk8hM9A1jxoyxdevW2eTJk11mMmjQIFc6CjvnLV++3I0uCD388MNuBMJ3vvOdCp+ja0H87Gc/s2Zt/5Fmw39s9tE0s+fGm+3YZDYkyGwBJCal8g4Aze86MMmQ1Os5lBSbvXyt2d9/Hzw/4Qazk28KmpmAFOHjNVV8TDMQNfnN5TowKSkjK+gPo8BF3r3L7P+uCgIbAACQFAQwtaHallN+anb2fWZp6Waf/t7s6QvNCmmLBwAgGQhgEqH+Lxf80SyzpdmiN82mf8tsW9X3cQEAAI2HACZRB55pdsnLwf2SVn9q9vhpZhsWJztVAACkFAKYuug1xOzSN8069DPbtDQIYr75JNmpAlBX9GkDvEMAU1f7DAiCmO6Dgps/Tj/bbOHMZKcKQKJWzjO7f3AwB+ANApj6aNPF7JJXzPYbabZrh9kzF5rNm57sVAFIxOz7zDYvM/vjBWabuX8S4AsCmPrKaWN24TNmgy4yi5Wa/eWaoDZm8V/Nmv8ldgCMfsis62FmBWvN/vg9s535yU4RgFoggGmoa8V8+0GzkyaapWeaLX3P7Hfnmj16ktkXL+mmL8lOIYDq5LQ1+/4Mszbdgnuf6arbJbuSnSoAe0EA05DXijnpRrP//jS4/YCGWmuU0rMXmz10tNmnT9NREGiucnuZff8Zs6xWZoveMnvtBmpQgWaOAKahte9tduadZtd9bnb89WY5uWbrF5q9+GOzXx9lNvcxs+IdyU4lgMp6HGl23mMqjZh98rjZhw8lO0UAakAA01hadzI7dZLZdZ+ZnTrFrHVnsy3LzV693uy+w81m/5K2dqC5Ofhss9NvCx6//lOzL19NdooAVIMAprG1yDU7foLZtZ+ZnXWPWW5vs4J1Zm/9zOye/c1+d57ZnAfN1i2kyhpoDkZcZTZYd52Pmf35UrNVnyY7RQCqwN2om5r6wXz2XFADo6aleO16me13itmAU832PdGsZYdkpRLw/hysV5p1nmpEkkYTtu1udtkss9yejZVUILLyGzHvIIBJFm32tQvMFs8yWzTLbNkHZiWFu/+um0b2HGK236lBQNPzKLP0jGSmGCnMx3Ow3mneucXs8VFm6xaYdT3c7AevBSOWANQaAYyHmWfCirYHQUwY0FSunWnVKbgP08HnmO17kllmTrJSihTk4znYIGnevNzssVODa8TsPyq4mWtGZkMnFYisfAIY/zLPetMVQVV9rYDm63eC0mAou63Z/qcFwYzmlArRyHw8Bxsszd/MM5t+ltmuncElEjTKEECtEMB4mHk2KLXHL3vfbMHLZl++YrZ11e6/ZeSYDTjZ7KCzzQ48y6z1PslMKSLKx3OwQdP8rxfN/jQueHzm3WbDf9ggaQSiLp8Axr/Ms9Hoqr6r5pst+EswbVxcsd9M32PNDvl2MOleTUCKnoMNnmZ1vNfoQZ1nx1xtdsINwa1EAFSLAMbDzLNJaNet+zKomVnwklneP3f/TZls/xPMDjs/qJ1p1TGZKYXnfDwHGzzNOt90HaePfxM8b9vDbNTPzQ49N7gSN4A9EMB4mHkmxaZlQa3Mv543Wzlv9/L0LLMBpwTBjDoCt2AbIvrnYKOleeHM4FYDuoO19D8xuMZT5wMa7juAiMgngPEv80y6jUuCQObzF8zWfFaxz8wBp5sdep7ZAWeYZbdKZirhCR/PwUZNs24H8v6vgmYlde5VIWHElWYn/A/NSkAcAhgPM89mRVf5/VzBzJ/NNvx793LduK7PCLN+xwWT7gWjO2sDETgHmyTNKijMnGj21WvB83Y9g2alQ0bTrAQYAYyXmWezpF295vMgkFFAE1aBxwc0vYeb9TvWrN/xZj2OMsvMTlZq0Yz4eA42aZorNyvpWk0arVRds1LJLrPC/ODyCJr0eJ/9zdp1b9x0Ak2MAMbDzNObYGbp+2bLZgfzHRsrviazpVnvoUEwo5qarofSGThF+XgONnmaq2pWUp8zXQYhDFTCqWjrnu/X64+62Oy4CcFd7YEIyCeA8S/z9HJ4tkY0LZ29O6DZvn7P1+m+MF0ONutySNn8YLPOB5llt05GqtFEfDwHk5bmys1KNclqHdzwVTWdm5bGBTJjzY7/f9x/Cd7LJ4DxL/OMxhDthWXBzOzgaqRbllfz4jSzDv12BzUd+gYBTXabsnmlx8q0uRy7V3w8B5OeZl1Be/U/gwBlj6l9MBowvs+ZzrN3/tds6XvB84xss6PGBXezb9ej6dOPaCgtCS6rkaQ+WQQwyc6IECjcarb2S7O1XwQ3ogznuk9MojJbVB3cVHgc/7xNECB1H2iW1aIx1g4ROwd9TLOz5D2zd6YGV98ORw4OvsTsuOvoI4PE/Psts5euMmvd2ey70832GdDkSSCA8TUjShUF63cHM2v+ZbY1z6x4u1nRNrOigrJpm1nhNrNYSf2+S9Xr3Y8w6zUs6J+jeW4vRnw0Mh/PQR/TXE7Z8pJ3g0Bm+ZzdgcyQ8UEg07ZbslOI5qyk2Oyvt5u9f9/uZTm5Zuc9anbgGU2aFAIYnzMi7KZDraSoYkBTVaBTOejR4x2bzFZ/alawrup+Ob2GlAU1w8y6D6KWpoH5eA76mOaqA5m/mb091WzFh7trL1UbqVJ1605lU+fgjvXxy/Sc8yD1bF5u9tylZt/MDZ4PHh8ULsPjR7fAOOlGs/SMJkkOAUwUMiLUnw5VdXT85uNgWjHXLO+zKmp10oI7dLupXdDXIHysuXteNqk/gjL9Np13/wgwdDwS56CPaa7x2P/67SCQCX+YakN9bfocHQzr1qQO99RWRteXr5i9+F9mOzcHNS7fvj+4L96uIrM3bjab+0jwuv1OC2pjmmBUKQFMlDIiNKyi7UHNjIKZMKipS5+ceGFQU16a1bzL7se6SWb4XK9NgR8EH89BH9O8V8quFbRvWRE03apGcvuGYO6msscaQVi6a8/3t+m2O5jZ98TEOgerM6iah/NXBTWnmTnBpKYt97hFxWXqoNyQ54bWXbWyO/ODmlylXd/VmPSdGh7vaoO3Vqw51sCFffZrHuf/rkKzNyebfTQteN5zsNl3ngjSGO8fM8z+co3Zrh1m7fuaXfAHs26HN2rSCGCimBGhcehwVsYeXiRMc2U8O8vm7rn+VjZX01SY6WtKtI+OMuow0IkPbNQvp0P/IAPRNT0aO6NtZD6egz6muUHPA5XCVWOpvjSL3w760uj6NPE6HRgEMwNODpqldA5s+SYIUvJXVnys4CWh8yMtCGrUjKWLZGa1LJtal83jl7UKzhEFBuG5WWGuc3mrWay04ue786zf7qlj2Tmnc6+q2oXinWbb8sy2rjHbutpsm+Z5u+fKD+Kbrosqf2clqrFVDZeuk6VJ/fOa+mrmGxabPTfebPU/gucjrjI7dUr1NckaGTfjP4OLLupaX+f8ymzgmMS+c8fm4HjqfKBZx31rfCkBTCpnRGjaa+Eo0y8vza7bXcrdtrbi8m3rqr4YWZXSgkvMx2e0bupr1r5PMMpKQ2bVQTk93ZojH89BH9PcqPTjveKjYHi3mqNWfapIJ7HPSMsIaj50zKrUr5oQBUW7yualxY2V+oppUJBQORirTLWjOs/UVKzzV4GLCjV15UZEtgnudaXAbMOiPdOggED98VxAc3TQJ0/N1vH7wOUhyk/W75mvqLZHae50gFmn/YOrM6tAVF0tz2fPmf3l2iAvatnBbPS02nXS3b7R7PnLzRa9FTwf9qPgFhjVBV9hwKKh/hrmryBIx84pk8xOuL7GryKAISNCc6TMJgxmwkwpzJA2rwhKv5qKC2r/memZQSCjgCYjfp4VZI768VCp0029dz9WR+ZGvLaOj+egj2luUvoR04+RAhrV0Kjzp44jXTxPx5mCbk3uedmkWsaaOn+qEFBSGAQ3blJwszOoWXFNMdt3Py6fF5TNdwY3ly3vtxbOcys+V22N6DzT+aULB25aUvGxalRqqjXVKC5NbbrGzbsHtTZhkJKtfnNlQYu+s3LhQuunWg/9sC//MJirBieerr+iWi5tk7BmOFHqx6RgRkGNmqz0uOOAoD/LvOnBaxQwnf94Yhc+VJOgrjv07l27P0NDrbU9qgtY4ikNwy43O/qKGr+GAIaMCL43aYXBTOVJVfOJloKrooyybVxw07J9pR+RuHn5D0zZc5Vor4u7Y3lEzkEf05z0Y7U59OdoCGr+2bQsOMfUJKTAS/1/2nYNAoLGWE8Fb+u/qhjQVL7fnKiA0ibsU9el4iACBVcbF5ut/3dw410VhGrMH9KCKzafNLHuBZiFr5k9/8MguAoDuqoCFgVP7sa/x5v1PbbW1yQigCEjQlSpFKRqeDcVl03h47Ll6oypudrlw34J5dOKoI9CfaruVeszeUPkzkEf04yI0bmZ93lQkxMODEik479qptTHRYGRmqwU2ISPW3UM+q8MOKX+6dR3PHOR2boF9Q5YmvI85HruQDKpOj69rCNjfUp+ar4KAxrN1fGxwsiQFpUeVxo1EqXSN9BcuKa4etwGQvlCt8OCKV5Y75DWQOesrtB72Vtmnz8XNJnVI2BpSgQwgO/UNh+26asDIYBoa4zCRk6b4JYVHmmeQx4AAABqQAADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC8QwADAAC848XdqGNltw7Pz89PdlKAlBSee+G56APyDSDaeYcXAczWrVvdvHfv3slOCpDSdC7m5uaaD8g3gGjnHWkxD4pUpaWltmrVKmvbtq2lpaXVGOkps1qxYoW1a9fOooL18ksU10vZhDKgHj16WHq6Hy3Ptc03orrPorhOwnr5JdaIeYcXNTBa6V69etX69dr5UToAQqyXX6K2Xr7UvNQ134jiPovqOgnr5Y/Gyjv8KEoBAADEIYABAADeiVQAk5OTY1OmTHHzKGG9/BLV9YqyKO6zKK6TsF7wqhMvAABAZGtgAABAaiCAAQAA3iGAAQAA3iGAAQAA3olMAPPggw9av379rEWLFjZ8+HCbO3eu+e5nP/uZu4Jo/HTQQQeZb959910755xz3JUYtQ4vvvhihb+rH/nkyZOte/fu1rJlSxs5cqT9+9//Nt/X65JLLtlj/51xxhlJSy9SI+8g32jeyDcaTiQCmBkzZtiECRPcELT58+fbwIEDbdSoUbZ27Vrz3aGHHmqrV68un2bPnm2+KSgocPtEPxRVueuuu+zXv/61TZs2zT766CNr3bq12387d+40n9dLlPHE77+nn366SdOI1Mw7yDeaL/KNBhSLgGHDhsWuvPLK8uclJSWxHj16xKZOnRrz2ZQpU2IDBw6MRYkOuRdeeKH8eWlpaaxbt26xu+++u3zZ5s2bYzk5ObGnn3465ut6ybhx42Lf/va3k5YmpGbeQb5BvpEqvK+BKSoqsnnz5rnqw/h7oOj5nDlzzHeqElVV47777msXXXSRLV++3KJkyZIllpeXV2H/6b4ZqsqPwv575513rEuXLnbggQfaFVdcYRs2bEh2kpACeQf5ht/IN2rH+wBm/fr1VlJSYl27dq2wXM91gPtMJ+P06dNt5syZ9vDDD7uT9vjjj3d39oyKcB9Fcf+pGvipp56yWbNm2Z133ml/+9vf7Mwzz3THK5IvqnkH+Ya/+07INyJ2N+pUpYM2dMQRR7iMqW/fvvbss8/apZdemtS0Ye8uuOCC8seHH36424cDBgxwpatTTz01qWlDdJFv+I18I4VqYDp16mQZGRm2Zs2aCsv1vFu3bhYl7du3twMOOMAWLVpkURHuo1TYf6rO1/Eapf3ns1TJO8g3/Ea+EeEAJjs72wYPHuyq20KlpaXu+YgRIyxKtm3bZosXL3bDBqOif//+LsOJ33/5+fluVEHU9t8333zj2rKjtP98lip5B/mG38g3It6EpGGQ48aNsyFDhtiwYcPsvvvuc0PVxo8fbz67/vrr3fUCVP27atUqN9RTJcYLL7zQfMtA40sPapP/9NNPrWPHjtanTx+79tpr7fbbb7f999/fZUyTJk1yHRBHjx5tvq6XpltuucXOP/98l9HqB+SGG26w/fbbzw31RPMQxbyDfIN8I2XEIuL++++P9enTJ5adne2GRn744Ycx340ZMybWvXt3t049e/Z0zxctWhTzzdtvv+2GC1aeNFwwHBI5adKkWNeuXd0wyFNPPTW2cOHCmM/rtX379tjpp58e69y5cywrKyvWt2/f2OWXXx7Ly8tLdrIR8byDfKN5I99oOGn6L9lBFAAAQEr1gQEAAKmHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAHiHAAYAAJhv/j83qT5Dqj6P3AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "historico.history.keys() # Mostra as métricas disponíveis\n",
    "\n",
    "plt.subplot(1, 2, 1) # ← (linhas, colunas, posição exibida 1)\n",
    "plt.title('Acurácia e Perda de Treinamento') # Título do gráfico\n",
    "plt.plot(historico.history['accuracy']) # Acurácia de treinamento\n",
    "plt.plot(historico.history['loss']) # Perda de treinamento \n",
    "\n",
    "plt.subplot(1, 2, 2) # ← (linhas, colunas, posição exibida 2)\n",
    "plt.title('Acurácia e Perda de Validação') # Título do gráfico\n",
    "plt.plot(historico.history['val_accuracy']) # Acurácia de validação\n",
    "plt.plot(historico.history['val_loss']) # Perda de validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.2760594e-13, 5.3495668e-08, 3.8849421e-08, ..., 9.9999869e-01,\n",
       "        5.4592279e-11, 1.0347462e-06],\n",
       "       [3.1930060e-12, 1.3204173e-05, 9.9997437e-01, ..., 5.9789227e-07,\n",
       "        2.8791234e-07, 8.9286692e-14],\n",
       "       [2.3940489e-12, 9.9999142e-01, 4.0788781e-10, ..., 5.2251489e-06,\n",
       "        2.1417834e-06, 1.6942698e-08],\n",
       "       ...,\n",
       "       [3.3294145e-11, 4.7291442e-11, 1.1475713e-09, ..., 7.2616365e-07,\n",
       "        1.6589013e-08, 1.3259991e-06],\n",
       "       [3.3194630e-12, 4.9020128e-14, 9.4598461e-13, ..., 1.6817367e-11,\n",
       "        3.6484284e-08, 3.0573158e-10],\n",
       "       [5.6296486e-14, 9.2224569e-19, 5.1544337e-16, ..., 1.5560752e-21,\n",
       "        1.4337551e-13, 1.0109818e-18]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = modelo.predict(X_teste) # Faz previsões com a partição de teste para avaliar o modelo \n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ...,  True, False, False],\n",
       "       [False, False,  True, ..., False, False, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = (previsoes > 0.5) # Converte as probabilidades em True (1) ou False (0)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegar o número de maior valor e gerar a Matriz de Confusão\n",
    "\n",
    "O código tem como objetivo avaliar o desempenho de um modelo de classificação que prevê dígitos de 0 a 9. Para isso, ele gera uma matriz de confusão, que mostra o quão bem o modelo está classificando cada dígito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 972,    1,    1,    0,    0,    1,    2,    1,    2,    0],\n",
       "       [   3, 1122,    3,    1,    0,    0,    2,    1,    3,    0],\n",
       "       [   6,    2,  996,    8,    3,    0,    1,    7,    9,    0],\n",
       "       [   8,    0,    3,  988,    0,    7,    0,    3,    1,    0],\n",
       "       [   4,    0,    2,    0,  957,    0,    7,    2,    0,   10],\n",
       "       [   5,    0,    0,    8,    1,  868,    2,    2,    3,    3],\n",
       "       [   8,    3,    0,    0,    4,    6,  935,    0,    2,    0],\n",
       "       [   9,    4,    7,    1,    7,    0,    0,  997,    0,    3],\n",
       "       [  12,    1,    4,    7,    6,    6,    0,    3,  928,    7],\n",
       "       [   8,    3,    0,    4,   11,    3,    0,    8,    0,  972]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste_matriz = [np.argmax(t) for t in y_teste] # Converte as probabilidades em valores de 0 a 9\n",
    "y_previsoes_matriz = [np.argmax(t) for t in previsoes] # Converte as probabilidades em valores de 0 a 9\n",
    "confusao = confusion_matrix(y_teste_matriz, y_previsoes_matriz) # Cria a matriz de confusão\n",
    "confusao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer previsões reais a partir do modelo criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Número a prever:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ]\")\n",
    "print(y_treinamento[0])\n",
    "print('Número a prever: ', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "O número previsto é: 5\n"
     ]
    }
   ],
   "source": [
    "novo = X_treinamento[0]  # Pega a primeira imagem da base de treinamento\n",
    "novo = np.expand_dims(novo, axis=0)  # Adiciona uma dimensão extra para a predição\n",
    "pred = modelo.predict(novo)  # Faz a predição\n",
    "pred = [np.argmax(pred) for p in pred]  # Converte as probabilidades em valores de 0 a 9\n",
    "\n",
    "pred = int(pred[0])  # Acessa o primeiro elemento da lista e converte para inteiro\n",
    "print(\"O número previsto é:\", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
